# -*- coding: utf-8 -*-
"""deep learning - lab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10s3Kqa-n-cAjmPuuCXX7cdZ0BJZ54uAH


from google.colab import drive
drive.mount('/content/gdrive')

!ls "/content/gdrive/My Drive/Deep learning - lab"

!unzip -q "/content/gdrive/My Drive/Deep learning - lab/data_1.zip"
!ls
"""
# Import some packages to use

import os
import random

import gc  # Gabage collector for cleaning deleted data from memory
from keras import Sequential
from keras import backend as K
from keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dropout, Dense
import keras
# Extract filenames

CATEGORY_1 = 'dog'
CATEGORY_2 = 'kitten'
DATA_DIR = 'data_1'

train_dir = os.path.join(os.getcwd(), DATA_DIR, 'train')
test_dir = os.path.join(os.getcwd(), DATA_DIR, 'test')

train_cat_1 = []
train_cat_2 = []
test_imgs = []

for img in os.listdir(train_dir):
    if CATEGORY_1 in img:
        train_cat_1.append(f'{train_dir}/{img}')

for img in os.listdir(train_dir):
    if CATEGORY_2 in img:
        train_cat_2.append(f'{train_dir}/{img}')

train_imgs = train_cat_1 + train_cat_2

for img in os.listdir(test_dir):
    test_imgs.append(f'{test_dir}/{img}')

random.shuffle(train_imgs)  # shuffle it randomly
gc.collect()   # collect garbage to save memory

import cv2
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Lets declare our image dimensions; we are using coloured images.
nrows = 224
ncolumns = 224
channels = 3  # change to 1 if you want to use grayscale image

# A function to read and process the images to an acceptable format for our model
# CATEGORY_1 - 1, CATEGORY_2 - 0
def read_and_process_image(list_of_images):
    """
    Returns two arrays:
        X is an array of resized images
        y is an array of labels
    """
    X = []  # images
    y = []  # labels

    for image in list_of_images:
        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (nrows, ncolumns),
                            interpolation=cv2.INTER_CUBIC))  # Read the image
        # get the labels
        if CATEGORY_1 in image:
            y.append(1)
        elif CATEGORY_2 in image:
            y.append(0)

    return X, y

# get the train and label data
X, y = read_and_process_image(train_imgs)

# Lets view some of the pics
plt.figure(figsize=(20,10))
columns = 5
for i in range(columns):
    plt.subplot(5 / columns + 1, columns, i + 1)
    plt.imshow(X[i])
plt.show()

del train_imgs
gc.collect()

# Convert list to numpy array
X = np.array(X)
y = np.array(y)

# Lets plot the label to be sure we just have two class
sns.countplot(y)
plt.title(f'Labels for {CATEGORY_1} and {CATEGORY_2}')
plt.show()

# Split the data into train and test set
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=2)

del X
del y
gc.collect()

# get the length of the train and validation data
ntrain = len(X_train)
nval = len(X_val)

print(ntrain)
print(nval)

# We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
import keras.backend as K
batch_size = 32

if K.image_data_format() == 'channels_first':
    input_shape = (3, nrows, ncolumns)
else:
    input_shape = (nrows, ncolumns, 3)


# Lets create the augmentation configuration
# This helps prevent overfitting, since we are using a small dataset
from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255,   # Scale the image between 0 and 1
                                    rotation_range=40,
                                    width_shift_range=0.2,
                                    height_shift_range=0.2,
                                    shear_range=0.2,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    fill_mode='nearest')

val_datagen = ImageDataGenerator(rescale=1./255)  # We do not augment validation data. we only perform rescale

# Create the image generators
train_generator = train_datagen.flow(X_train, y_train,batch_size=batch_size)
val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)

# The training part
# We train for 30 epochs with about 25 steps per epoch
model = Sequential()

model.add(Conv2D(filters=32, 
               kernel_size=(2,2), 
               strides=(1,1),
               padding='same',
               input_shape=(224,224,3),
               data_format='channels_last'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2),
                     strides=2))

model.add(Dropout(0.4))

model.add(Conv2D(filters=64,
               kernel_size=(2,2),
               strides=(1,1),
               padding='valid'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2),
                     strides=2))

model.add(Flatten())        
model.add(Dense(64))
model.add(Activation('relu'))

model.add(Dropout(0.4))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss = 'binary_crossentropy', 
            optimizer = 'adam', metrics = ['acc'])

train_generator = train_datagen.flow(X_train, y_train,batch_size=32)
val_generator = val_datagen.flow(X_val, y_val, batch_size=32)


early_stop = keras.callbacks.EarlyStopping(patience=80) 
callbacks = [early_stop] 

history = model.fit_generator(train_generator,
                              steps_per_epoch=32,
                              epochs=600,
                              validation_data=val_generator,
                              validation_steps=32, callbacks=callbacks)


# Plot the train and val curve
import matplotlib.pyplot as plt

# Get the details form the history object
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# Train and validation accuracy
plt.plot(epochs, acc, 'b', label='Training accurarcy')
plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
plt.title('Training and Validation accurarcy')
plt.legend()
plt.show()

plt.figure()
# Train and validation loss
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation loss')
plt.legend()

plt.show()

def smooth_plot(points, factor=0.7):
    smooth_pts = []
    for point in points:
        if smooth_pts:
            previous = smooth_pts[-1]
            smooth_pts.append(previous * factor + point * (1 - factor))
        else:
            smooth_pts.append(point)
    return smooth_pts

# Plot figure
plt.plot(epochs, smooth_plot(acc), 'b', label='Training accurarcy')
plt.plot(epochs, smooth_plot(val_acc), 'r', label='Validation accurarcy')
plt.title('Training and Validation accurarcy')
plt.legend()
plt.show()

# Now lets predict on the first 10 Images of the test set
X_test, y_test = read_and_process_image(test_imgs) # Y_test in this case will be empty.
x = np.array(X_test)
test_datagen = ImageDataGenerator(rescale=1./255)


i = 0
columns = 5
text_labels = []
plt.figure(figsize=(30,20))
for batch in test_datagen.flow(x, batch_size=1):
    pred = model.predict(batch)
    if pred > 0.5:
        text_labels.append(CATEGORY_1)
    else:
        text_labels.append(CATEGORY_2)
    plt.subplot(5 / columns + 1, columns, i + 1)
    plt.title('This is a ' + text_labels[i])
    imgplot = plt.imshow(batch[0])
    i += 1
    if i % 10 == 0:
        break
plt.show()

X_test = np.array(X_test)/255
pred = np.round(model.predict(X_test))
print("Accuracy: ", accuracy_score(y_test, pred) )